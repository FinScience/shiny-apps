shiny::runApp()
pages <- list("Douglas" = "douglas",
"Flaconi" = "flaconi",
"Pieper" = "pieper",
"Parfumdreams" = "parfumdreams",
"iparfumerie" = "iparfumerie")
shiny::runApp()
################# ~~~~~~~~~~~~~~~~~ ######## ~~~~~~~~~~~~~~~~~ #################
##                                                                            ##
##                        Text Mining of Facebook Posts                       ##
##                                                                            ##
##                    App & Code by Maximilian H. Nierhoff                    ##
##                                                                            ##
##                           http://nierhoff.info                             ##
##                                                                            ##
##         Live version of this app: https://nierhoff.shinyapps.io/TTMA       ##
##                                                                            ##
##         Github Repo for this app: https://github.com/mhnierhoff/TTMA       ##
##                                                                            ##
################# ~~~~~~~~~~~~~~~~~ ######## ~~~~~~~~~~~~~~~~~ #################
library(Rfacebook)
library(NLP)
library(tm)
library(SnowballC)
library(slam)
library(RWeka)
library(rJava)
library(RWekajars)
library(memoise)
posts <- readLines(sprintf("douglas.csv"))
## Build the corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(posts))
## Make it work with the new tm package
myCorpus <- tm_map(myCorpus,
content_transformer(function(x) iconv(x, to="UTF-8-MAC", sub="byte")),
mc.cores=1)
## Convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower), lazy = TRUE)
## Remove punctuation
myCorpus <- tm_map(myCorpus, content_transformer(removePunctuation))
## Remove numbers
myCorpus <- tm_map(myCorpus, content_transformer(removeNumbers))
## Remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
## Remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, c(stopwords("german"), "wwwpinterestcompin"))
## Final corpus
mytdm <- TermDocumentMatrix(myCorpus)
mytdm
tm <- as.DocumentTermMatrix(mytdm)
rowTotals <- apply(dtm , 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]
lda <- LDA(dtm.new, k = 4)
term <- terms(lda, 4)
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
topicPlot <- ggplot(topic, aes(x = term, fill = Term[topic]))
topicPlot + geom_density +
labs(title = "Most Frequent Terms") +
labs(y = "Terms") +
labs(x = "Count") +
coord_flip() +
theme_bw()
tm <- as.DocumentTermMatrix(mytdm)
rowTotals <- apply(dtm, 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]
lda <- LDA(dtm.new, k = 4)
term <- terms(lda, 4)
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
topicPlot <- ggplot(topic, aes(x = term, fill = Term[topic]))
topicPlot + geom_density +
labs(title = "Most Frequent Terms") +
labs(y = "Terms") +
labs(x = "Count") +
coord_flip() +
theme_bw()
################# ~~~~~~~~~~~~~~~~~ ######## ~~~~~~~~~~~~~~~~~ #################
##                                                                            ##
##                        Text Mining of Facebook Posts                       ##
##                                                                            ##
##                    App & Code by Maximilian H. Nierhoff                    ##
##                                                                            ##
##                           http://nierhoff.info                             ##
##                                                                            ##
##         Live version of this app: https://nierhoff.shinyapps.io/TTMA       ##
##                                                                            ##
##         Github Repo for this app: https://github.com/mhnierhoff/TTMA       ##
##                                                                            ##
################# ~~~~~~~~~~~~~~~~~ ######## ~~~~~~~~~~~~~~~~~ #################
library(Rfacebook)
library(NLP)
library(tm)
library(SnowballC)
library(slam)
library(RWeka)
library(rJava)
library(RWekajars)
library(memoise)
posts <- readLines(sprintf("douglas.csv"))
## Build the corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(posts))
## Make it work with the new tm package
myCorpus <- tm_map(myCorpus,
content_transformer(function(x) iconv(x, to="UTF-8-MAC", sub="byte")),
mc.cores=1)
## Convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower), lazy = TRUE)
## Remove punctuation
myCorpus <- tm_map(myCorpus, content_transformer(removePunctuation))
## Remove numbers
myCorpus <- tm_map(myCorpus, content_transformer(removeNumbers))
## Remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
## Remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, c(stopwords("german"), "wwwpinterestcompin"))
## Final corpus
mytdm <- TermDocumentMatrix(myCorpus)
tm <- as.DocumentTermMatrix(mytdm)
lda <- LDA(dtm, k = 4)
term <- terms(lda, 4)
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
topicPlot <- ggplot(topic, aes(x = term, fill = Term[topic]))
topicPlot + geom_density +
labs(title = "Most Frequent Terms") +
labs(y = "Terms") +
labs(x = "Count") +
coord_flip() +
theme_bw()
tm <- as.DocumentTermMatrix(mytdm)
tm
rowTotals <- apply(mytdm, 1, sum)
rowTotals
dtm.new   <- dtm[rowTotals> 0, ]
dtm.new   <- rowTotals > 0
lda <- LDA(dtm.new, k = 4)
dtm.new   <- mytdm[rowTotals> 0, ]
lda <- LDA(dtm.new, k = 4)
lda
term <- terms(lda, 4)
term
term <- terms(lda, 4)
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
topic
topicPlot <- ggplot(topic, aes(x = term, fill = Term[topic]))
topicPlot + geom_density +
labs(title = "Most Frequent Terms") +
labs(y = "Terms") +
labs(x = "Count") +
coord_flip() +
theme_bw()
plot(topic)
barplot(table(topic))
barplot(table(x))
barplot(table(term))
term
topicPlot <- ggplot(term, aes(x = topic, fill = Term[topic]))
topicPlot + geom_density +
labs(title = "Most Frequent Terms") +
labs(y = "Terms") +
labs(x = "Count") +
coord_flip() +
theme_bw()
print(iris_ctree)
plot(iris_ctree)
shiny::runApp()
inspect(mytdm[idx+(0:5),101:110])
idx <- which(dimnames(mytdm)$Terms == "douglas")
inspect(mytdm[idx+(0:5),101:110])
install.packages(c("googleVis", "manipulate", "NLP", "psych", "Rcpp", "RcppArmadillo", "rmarkdown", "RWeka", "RWekajars", "timeDate", "timeSeries"))
install.packages(c("googleVis", "manipulate", "NLP", "psych",
)
)
install.packages(c("googleVis", "manipulate", "NLP", "psych", "Rcpp", "RcppArmadillo", "rmarkdown", "RWeka", "RWekajars", "timeDate", "timeSeries"))
install.packages(c("googleVis", "manipulate", "NLP", "psych",
""))
install.packages(c("googleVis", "manipulate", "NLP", "psych", "Rcpp", "RcppArmadillo", "rmarkdown", "RWeka", "RWekajars", "timeDate", "timeSeries"))
install.packages(c("googleVis", "manipulate", "NLP", "psych",
detach("package:datasets", unload=TRUE)
detach("package:graphics", unload=TRUE)
detach("package:grDevices", unload=TRUE)
detach("package:methods", unload=TRUE)
detach("package:stats", unload=TRUE)
detach("package:utils", unload=TRUE)
detach("package:topicmodels", unload=TRUE)
install.packages(c("Rcpp", "RcppArmadillo", "rmarkdown", "RWeka", "RWekajars", "timeDate", "timeSeries"))
library("utils", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
install.packages(c("Rcpp", "RcppArmadillo", "rmarkdown", "RWeka", "RWekajars", "timeDate", "timeSeries"))
shiny::runApp()
shiny::runApp()
suppressPackageStartupMessages(c(
library(graph),
library(twitteR),
library(NLP),
library(tm),
library(shinyIncubator),
library(grid),
library(pvclust),
library(Rgraphviz),
library(qdapTools),
library(qdapRegex),
library(magrittr),
library(wordcloud),
library(RColorBrewer),
library(ggplot2),
library(RCurl),
library(bitops),
library(BH),
library(ape),
library(topicmodels),
library(qdap)))
shiny::runApp()
shiny::runApp()
shiny::runApp()
install.packages("matrixStats")
shiny::runApp()
shiny::runApp()
################# ~~~~~~~~~~~~~~~~~ ######## ~~~~~~~~~~~~~~~~~ #################
##                                                                            ##
##                        Text Mining of Facebook Posts                       ##
##                                                                            ##
##                    App & Code by Maximilian H. Nierhoff                    ##
##                                                                            ##
##                           http://nierhoff.info                             ##
##                                                                            ##
##         Live version of this app: https://nierhoff.shinyapps.io/TTMA       ##
##                                                                            ##
##         Github Repo for this app: https://github.com/mhnierhoff/TTMA       ##
##                                                                            ##
################# ~~~~~~~~~~~~~~~~~ ######## ~~~~~~~~~~~~~~~~~ #################
library(Rfacebook)
library(NLP)
library(tm)
library(SnowballC)
library(slam)
library(RWeka)
library(rJava)
library(RWekajars)
library(memoise)
posts <- readLines(sprintf("douglas.csv"))
## Build the corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(posts))
## Make it work with the new tm package
myCorpus <- tm_map(myCorpus,
content_transformer(function(x) iconv(x, to="UTF-8-MAC", sub="byte")),
mc.cores=1)
## Convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower), lazy = TRUE)
## Remove punctuation
myCorpus <- tm_map(myCorpus, content_transformer(removePunctuation))
## Remove numbers
myCorpus <- tm_map(myCorpus, content_transformer(removeNumbers))
## Remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
## Remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, c(stopwords("german"), "wwwpinterestcompin"))
## Final corpus
mytdm <- TermDocumentMatrix(myCorpus)
rowTotals <- apply(mytdm, 1, sum)
dtm.new   <- mytdm[rowTotals> 0, ]
lda <- LDA(dtm.new, k = 4)
term <- terms(lda, 4)
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
topicPlot <- ggplot(term, aes(x = topic, fill = Term[topic]))
topicPlot + geom_density +
labs(title = "Most Frequent Terms") +
labs(y = "Terms") +
labs(x = "Count") +
coord_flip() +
theme_bw()
idx <- which(dimnames(mytdm)$Terms == "douglas")
inspect(mytdm[idx+(0:5),101:110])
shiny::runApp()
################# ~~~~~~~~~~~~~~~~~ ######## ~~~~~~~~~~~~~~~~~ #################
##                                                                            ##
##                        Text Mining of Facebook Posts                       ##
##                                                                            ##
##                    App & Code by Maximilian H. Nierhoff                    ##
##                                                                            ##
##                           http://nierhoff.info                             ##
##                                                                            ##
##         Live version of this app: https://nierhoff.shinyapps.io/TTMA       ##
##                                                                            ##
##         Github Repo for this app: https://github.com/mhnierhoff/TTMA       ##
##                                                                            ##
################# ~~~~~~~~~~~~~~~~~ ######## ~~~~~~~~~~~~~~~~~ #################
library(Rfacebook)
load("fb_oauth")
## Get n posts of the accounts
Douglas_posts <- getPage(page="DouglasDeutschland",
token=fb_oauth, feed = FALSE, n=500)
Flaconi_posts <- getPage(page="flaconi.de",
token=fb_oauth, feed = FALSE, n=500)
Pieper_posts <- getPage(page="Parfuemerie.Pieper",
token=fb_oauth, feed = FALSE, n=500)
Parfumdreams_posts <- getPage(page="parfumdreams",
token=fb_oauth, feed = FALSE, n=500)
iparfumerie_posts <- getPage(page="iparfumerie.de",
token=fb_oauth, feed = FALSE, n=500)
PointRouge_posts <- getPage(page="PointRougeDeutschland",
token=fb_oauth, feed = FALSE, n=500)
## Save post message column as .csv file
write.csv(Douglas_posts[,3], file = "./douglas.csv")
write.csv(Flaconi_posts[,3], file = "./flaconi.csv")
write.csv(Pieper_posts[,3], file = "./pieper.csv")
write.csv(Parfumdreams_posts[,3], file = "./parfumdreams.csv")
write.csv(iparfumerie_posts[,3], file = "./iparfumerie.csv")
write.csv(PointRouge_posts[,3], file = "./pointrouge.csv")
PointRouge_posts <- getPage(page="PointRougeDeutschland",
token=fb_oauth, feed = FALSE, n=500)
shiny::runApp()
shiny::runApp()
shiny::runApp()
i <- read.csv(file = ".data/iparfumerie.csv", sep=",", header = TRUE)
i <- read.csv(file = "./data/iparfumerie.csv", sep=",", header = TRUE)
shiny::runApp()
